{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBZ5eKc2_KSd"
   },
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVYe7_jyJZ5I"
   },
   "outputs": [],
   "source": [
    "!pip install xlstm\n",
    "!pip install Ninja\n",
    "!pip install torch-summary\n",
    "!pip install --upgrade yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTlTCuiI_QVF"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIActA9KbELE"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from xlstm import (\n",
    "    xLSTMBlockStack,\n",
    "    xLSTMBlockStackConfig,\n",
    "    mLSTMBlockConfig,\n",
    "    mLSTMLayerConfig,\n",
    "    sLSTMBlockConfig,\n",
    ")\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torchsummary import summary\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "\n",
    "seed = 211\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWKbXLaE_TjW"
   },
   "source": [
    "# General Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lb_uyterVyma"
   },
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"data\": {\n",
    "        \"rolling_window\": 10,\n",
    "        \"lookback\": 100,\n",
    "        \"horizon\": 1,\n",
    "        \"split_ratios\": {'train': 0.64, 'val': 0.16, 'test': 0.2},\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_features\": 6,\n",
    "        \"num_blocks\": 3,\n",
    "        \"embedding_dim\": 128,\n",
    "        \"context_length\": 336,\n",
    "        \"sma_ksize\": 25\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 50,\n",
    "        \"lr\": 1e-4,\n",
    "        \"min_lr\": 1e-7,\n",
    "        \"sched_factor\": 0.5,\n",
    "        \"sched_threshold\": 1e-5\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoTErDooTTGU"
   },
   "outputs": [],
   "source": [
    "# Commonly used constants\n",
    "offset = config_dict['data']['rolling_window'] // 2\n",
    "lookback = config_dict['data']['lookback']\n",
    "horizon = config_dict['data']['horizon']\n",
    "batch_size = config_dict['training']['batch_size']\n",
    "epochs = config_dict['training']['epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxu0nHmw_e2B"
   },
   "source": [
    "# Submodule and Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPwJptDiktnF"
   },
   "outputs": [],
   "source": [
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM3kLDPRsLd1"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, seq_len, fcast_horizon, emb_dim2, sma_ksize, num_features):\n",
    "        self.seq_len = seq_len\n",
    "        self.fcast_horizon = fcast_horizon\n",
    "        self.emb_dim2 = emb_dim2\n",
    "        self.sma_ksize = sma_ksize\n",
    "        self.num_features = num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZuphWE4sNlC"
   },
   "outputs": [],
   "source": [
    "class SMA(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple moving average (SMA) block to capture the trend\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SMA, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pad both ends of the time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        back = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, back], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1)) # Averaging is done on the 3rd dim, but we want the 2nd\n",
    "        x = x.permute(0, 2, 1) # Permute back to original dimensions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaVm2cmBsQQ5"
   },
   "outputs": [],
   "source": [
    "class decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Decomposition block for seasonal and trend\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(decomp, self).__init__()\n",
    "        self.sma = SMA(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend = self.sma(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGwA6tUnTmrf"
   },
   "outputs": [],
   "source": [
    "class xlstm(nn.Module):\n",
    "    def __init__(self, cfg, bs_cfg):\n",
    "        super(xlstm, self).__init__()\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(cfg.num_features)\n",
    "\n",
    "        # Seasonal/Trend layers\n",
    "        self.decomp = decomp(kernel_size=cfg.sma_ksize)\n",
    "        self.LinearSeasonal = nn.Linear(cfg.seq_len, cfg.fcast_horizon)\n",
    "        self.LinearTrend = nn.Linear(cfg.seq_len, cfg.fcast_horizon)\n",
    "        self.LinearDecoder = nn.Linear(cfg.seq_len, cfg.fcast_horizon)\n",
    "\n",
    "        self.LinearSeasonal.weight = nn.Parameter(\n",
    "            (1 / cfg.seq_len) * torch.ones([cfg.fcast_horizon, cfg.seq_len])\n",
    "        )\n",
    "        self.LinearTrend.weight = nn.Parameter(\n",
    "            (1 / cfg.seq_len) * torch.ones([cfg.fcast_horizon, cfg.seq_len])\n",
    "        )\n",
    "\n",
    "        self.mm1 = nn.Linear(cfg.fcast_horizon, cfg.emb_dim2)\n",
    "        self.mm2 = nn.Linear(bs_cfg.embedding_dim, cfg.fcast_horizon)\n",
    "        self.mm3 = nn.Linear(cfg.seq_len, cfg.emb_dim2)\n",
    "\n",
    "        self.revin = RevIN(cfg.num_features)\n",
    "\n",
    "        self.xlstm_stack = xLSTMBlockStack(bs_cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.revin(x, 'norm')\n",
    "\n",
    "        seasonal, trend = self.decomp(x)\n",
    "        seasonal, trend = seasonal.permute(0, 2, 1), trend.permute(0, 2, 1)\n",
    "\n",
    "        seasonal_out = self.LinearSeasonal(seasonal)\n",
    "        trend_out = self.LinearTrend(trend)\n",
    "        x = seasonal_out + trend_out\n",
    "\n",
    "        x = self.mm1(x)\n",
    "        x = self.xlstm_stack(x)\n",
    "        x = self.mm2(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.revin(x, 'denorm')\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrFlWstse3Fd"
   },
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    def __init__(self, company, ticker):\n",
    "        self.company = company\n",
    "        self.ticker = ticker\n",
    "        self.model_stats = None\n",
    "        self.trained = False\n",
    "        self.has_all_predictions = False\n",
    "        self.has_set_indices = False\n",
    "        self.anim = None\n",
    "        self.losses = {'Train': None, 'Validation': None, 'Test': None}\n",
    "        self.__download_and_preprocess_data()\n",
    "        self.__create_set_tensors()\n",
    "        self.__init_model()\n",
    "\n",
    "\n",
    "    def __download_and_preprocess_data(self):\n",
    "        df_raw = yf.download(tickers=self.ticker, progress=False)\n",
    "        df_raw = df_raw.reset_index(drop=True)\n",
    "        # Check for 'Adj Close' column, or fallback to 'Close'\n",
    "        if 'Adj Close' in df_raw.columns:\n",
    "            df_raw = df_raw[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"]]\n",
    "        else:\n",
    "            print(\"Warning: 'Adj Close' column not found, using 'Close' instead.\")\n",
    "            df_raw = df_raw[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "            df_raw['Adj Close'] = df_raw['Close']  # Create 'Adj Close' column from 'Close'\n",
    "        df_raw.to_csv(self.ticker, index=False, header=False)\n",
    "        df = pd.read_csv(self.ticker, header=None)\n",
    "        df = df.rolling(config_dict['data']['rolling_window']).mean()\n",
    "        df.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "        # Convert price series to log\n",
    "        for column in df.columns:\n",
    "            if column != \"Volume\":\n",
    "                df[column] = np.log(df[column])\n",
    "\n",
    "        dataset = df.values.astype('float32')\n",
    "\n",
    "        self.df_raw = df_raw\n",
    "        self.df = df\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.dataset = self.scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "    def __create_features_labels(self, dataset, lookback, horizon):\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataset) - lookback - horizon):\n",
    "            X.append(dataset[i: i + lookback])\n",
    "            y.append(dataset[i + lookback: i + lookback + horizon])\n",
    "\n",
    "        return torch.from_numpy(np.array(X)).float(), torch.from_numpy(np.array(y)).float()\n",
    "\n",
    "\n",
    "    def __create_set_tensors(self):\n",
    "        full_size = len(self.dataset)\n",
    "        train_size = int(full_size * config_dict[\"data\"][\"split_ratios\"]['train'])\n",
    "        val_size = int(full_size * config_dict[\"data\"][\"split_ratios\"]['val'])\n",
    "\n",
    "        train_set = self.dataset[:train_size, :]\n",
    "        val_set = self.dataset[train_size: train_size + val_size, :]\n",
    "        test_set = self.dataset[train_size + val_size:, :]\n",
    "\n",
    "        X_train, y_train = self.__create_features_labels(train_set, lookback, horizon)\n",
    "        X_val, y_val = self.__create_features_labels(val_set, lookback, horizon)\n",
    "        X_test, y_test = self.__create_features_labels(test_set, lookback, horizon)\n",
    "\n",
    "        self.train_set_tensor = TensorDataset(X_train, y_train)\n",
    "        self.val_set_tensor = TensorDataset(X_val, y_val)\n",
    "        self.test_set_tensor = TensorDataset(X_test, y_test)\n",
    "\n",
    "        self.train_len = len(self.train_set_tensor)\n",
    "        self.val_len = len(self.val_set_tensor)\n",
    "        self.test_len = len(self.test_set_tensor)\n",
    "\n",
    "\n",
    "    def __init_data_loaders(self, shuffle_train=True):\n",
    "        batch_size = config_dict[\"training\"][\"batch_size\"]\n",
    "        self.train_loader = DataLoader(self.train_set_tensor, batch_size=batch_size, shuffle=shuffle_train)\n",
    "        self.val_loader = DataLoader(self.val_set_tensor, batch_size=batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(self.test_set_tensor, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    def __init_model(self):\n",
    "        block_stack_config = xLSTMBlockStackConfig(\n",
    "            mlstm_block = mLSTMBlockConfig(mlstm=mLSTMLayerConfig(bias=True)),\n",
    "            slstm_block = sLSTMBlockConfig(),\n",
    "            num_blocks = config_dict[\"model\"][\"num_blocks\"],\n",
    "            embedding_dim = config_dict[\"model\"][\"embedding_dim\"],\n",
    "            add_post_blocks_norm = True,\n",
    "            _block_map = 1,\n",
    "            context_length = config_dict[\"model\"][\"context_length\"],\n",
    "        )\n",
    "\n",
    "        config = Config(\n",
    "            seq_len = config_dict['data']['lookback'],\n",
    "            fcast_horizon = config_dict['data']['horizon'],\n",
    "            emb_dim2 = config_dict[\"model\"][\"embedding_dim\"],\n",
    "            sma_ksize = config_dict[\"model\"][\"sma_ksize\"],\n",
    "            num_features = config_dict[\"model\"][\"num_features\"]\n",
    "        )\n",
    "\n",
    "        self.model = xlstm(cfg=config, bs_cfg=block_stack_config).to(device)\n",
    "\n",
    "\n",
    "    def __init_train(self):\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "        self.optimizer = optim.RAdam(self.model.parameters(), lr=config_dict[\"training\"][\"lr\"])\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            factor = config_dict[\"training\"][\"sched_factor\"],\n",
    "            threshold = config_dict[\"training\"][\"sched_threshold\"],\n",
    "            min_lr=config_dict[\"training\"][\"min_lr\"],\n",
    "            patience=1\n",
    "        )\n",
    "\n",
    "\n",
    "    def train(self, verbose_progress=True):\n",
    "        self.__init_train()\n",
    "        self.__init_data_loaders()\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        epochs = config_dict[\"training\"][\"epochs\"]\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            running_train_loss = 0.0\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for i, (X_batch, y_batch) in enumerate(self.train_loader, 0):\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = self.model(X_batch)\n",
    "                train_loss = self.criterion(torch.exp(outputs[:, :, -1]), torch.exp(y_batch[:, :, -1]))\n",
    "                self.optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_train_loss += train_loss.item() * X_batch.size(0);\n",
    "\n",
    "            # Avg. training loss for the current epoch\n",
    "            epoch_train_loss = running_train_loss / len(self.train_loader.dataset)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "\n",
    "            # Validation phase\n",
    "            epoch_val_loss = self.__evaluate()\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            self.scheduler.step(epoch_val_loss)\n",
    "\n",
    "\n",
    "            # Print statistics\n",
    "            if verbose_progress and ((epoch + 1) % 5 == 0 or epoch == 0):\n",
    "                avg_epoch_time = (time.time() - start_time) / (epoch + 1)\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}]  |  '\n",
    "                      f'Avg. Epoch Time: {avg_epoch_time:.2f} sec  |  '\n",
    "                      f'Train Loss: {epoch_train_loss:.3E}  |  '\n",
    "                      f'Val Loss: {epoch_val_loss:.3E}  |  '\n",
    "                      f'LR: {self.scheduler.get_last_lr()[-1]:.3E}')\n",
    "\n",
    "        ### end for epoch in range(epochs)...\n",
    "\n",
    "        self.train_losses = train_losses\n",
    "        self.val_losses = val_losses\n",
    "        self.trained = True\n",
    "\n",
    "\n",
    "    def __evaluate(self):\n",
    "        self.model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for _, (X_batch, y_batch) in enumerate(self.val_loader, 0):\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                val_outputs = self.model(X_batch)\n",
    "                val_loss = self.criterion(torch.exp(val_outputs[:, :, -1]), torch.exp(y_batch[:, :, -1]))\n",
    "                running_val_loss += val_loss.item() * X_batch.size(0);\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(self.val_loader.dataset)\n",
    "        return epoch_val_loss\n",
    "\n",
    "\n",
    "    def __predict(self, data_loader, data_desc):\n",
    "        self.model.eval()\n",
    "\n",
    "        predicted_prices = np.array([])\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for _, (X_batch, y_batch) in enumerate(data_loader):\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                # Get predictions\n",
    "                preds = self.model(X_batch)\n",
    "                loss = self.criterion(torch.exp(preds[:, :, -1]), torch.exp(y_batch[:, :, -1]))\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "                preds = preds.cpu().numpy()\n",
    "                preds = np.array(\n",
    "                    [self.scaler.inverse_transform(preds[i]) for i in range(preds.shape[0])]\n",
    "                )\n",
    "\n",
    "                predicted_prices = np.concatenate((predicted_prices, np.exp(preds[:, -1, -1])))\n",
    "\n",
    "            total_loss = running_loss / len(data_loader.dataset)\n",
    "\n",
    "        self.losses[data_desc] = total_loss\n",
    "        return predicted_prices\n",
    "\n",
    "\n",
    "    def print_model_stats(self):\n",
    "        if self.model_stats is None:\n",
    "            self.model_stats = summary(\n",
    "                model = self.model,\n",
    "                dtypes = [torch.float, torch.float],\n",
    "                batch_dim = 1,\n",
    "                device = device,\n",
    "                verbose = 0\n",
    "            )\n",
    "\n",
    "        print(str(self.model_stats))\n",
    "\n",
    "\n",
    "    def plot_losses(self, from_epoch=0):\n",
    "        if self.trained == False:\n",
    "            raise AttributeError('Model has not been trained')\n",
    "\n",
    "        if from_epoch < 0 or from_epoch > epochs - 1:\n",
    "            raise ValueError(f'from_epoch must be in the range [0, {epochs - 1}]')\n",
    "\n",
    "        indices = np.arange(from_epoch, epochs)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(indices, self.train_losses[from_epoch:], label='Training Loss', color='blue')\n",
    "        plt.plot(indices, self.val_losses[from_epoch:], label='Validation Loss', color='red')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training and Validation Losses Vs. Epochs, Last {self.company}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def __get_all_predictions(self):\n",
    "        self.__init_data_loaders(shuffle_train=False)\n",
    "        loaders = [self.train_loader, self.val_loader, self.test_loader]\n",
    "        descriptions = ['Train', 'Validation', 'Test']\n",
    "        all_predictions = np.array([])\n",
    "        for i, loader in enumerate(loaders):\n",
    "            pred = self.__predict(loader, descriptions[i])\n",
    "            all_predictions = np.concatenate((all_predictions, pred))\n",
    "\n",
    "        self.all_predictions = all_predictions\n",
    "\n",
    "\n",
    "    def __get_set_indices(self):\n",
    "        self.train_begin_idx = lookback + offset\n",
    "        self.train_end_idx   = lookback + self.train_len + offset\n",
    "        self.val_begin_idx   = 2*lookback + self.train_len + offset\n",
    "        self.val_end_idx     = 2*lookback + self.train_len + self.val_len + offset\n",
    "        self.test_begin_idx  = 3*lookback + self.train_len + self.val_len + offset\n",
    "        self.test_end_idx    = 3*lookback + self.train_len + self.val_len + self.test_len + offset\n",
    "\n",
    "\n",
    "    def plot_set(self, sets):\n",
    "        if self.trained == False:\n",
    "            raise AttributeError('Model has not been trained')\n",
    "\n",
    "        if (sets != 'all') and (sets != 'test'):\n",
    "            raise ValueError(\"Argument 'sets' must be either 'all' or 'test'\")\n",
    "\n",
    "        if self.has_all_predictions == False:\n",
    "            self.__get_all_predictions()\n",
    "            self.has_all_predictions = True\n",
    "\n",
    "        if self.has_set_indices == False:\n",
    "            self.__get_set_indices()\n",
    "            self.has_set_indices = True\n",
    "\n",
    "        if sets == 'test':\n",
    "            print(f\"Test loss: {self.losses['Test']:.3E}\")\n",
    "\n",
    "        plt.figure(figsize=(25, 5))\n",
    "        if sets == 'test':\n",
    "            df_raw_test = self.df_raw[self.test_begin_idx: self.test_end_idx].reset_index(drop=True)\n",
    "            plt.plot(df_raw_test['Adj Close'], label='Actual Prices', color='orange')\n",
    "            plt.plot(self.all_predictions[self.train_len + self.val_len:], label='Forecasted Prices', color='magenta')\n",
    "            title_str = f'Forecasted & Actual Prices, xLSTM, 1d Horizon, Test Set, {self.company}'\n",
    "\n",
    "        else:\n",
    "            plt.plot(self.df_raw['Adj Close'],  label='Ground Truth Prices', color='orange')\n",
    "            plt.plot(np.arange(self.train_begin_idx, self.train_end_idx), self.all_predictions[:self.train_len], label='Train Forecasted Prices', color='purple')\n",
    "            plt.plot(np.arange(self.val_begin_idx, self.val_end_idx), self.all_predictions[self.train_len:self.train_len + self.val_len], label='Validation Forecasted Prices', color='blue')\n",
    "            plt.plot(np.arange(self.test_begin_idx, self.test_end_idx), self.all_predictions[self.train_len + self.val_len:], label='Test Forecasted Prices', color='magenta')\n",
    "            title_str = f'Forecasted & Actual Prices, xLSTM, 1d Horizon, All Sets, {self.company}'\n",
    "\n",
    "        plt.xlabel('Days')\n",
    "        plt.ylabel('Stock Price [USD]')\n",
    "        plt.title(title_str)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZMGNUEJpm9W"
   },
   "source": [
    "# Forecasting Coca-Cola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUh5zCvwn0AQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddw6QmjUPcS8"
   },
   "outputs": [],
   "source": [
    "KO_trainer = trainer('Coca Cola', 'KO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIuTpKKF4U7I",
    "outputId": "6da77f6f-ba48-4116-a23a-b39d6de104a1"
   },
   "outputs": [],
   "source": [
    "KO_trainer.print_model_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDO1p3Hc4TWw",
    "outputId": "bbd3ba11-6ef6-4f98-920c-23824721f262"
   },
   "outputs": [],
   "source": [
    "KO_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "hyWEVGS44WiH",
    "outputId": "28e207bd-4ba4-4608-8c78-c0c725cc1998"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "KO_trainer.plot_losses(from_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "F0NjhBP74Y2_",
    "outputId": "a89f3886-074a-4c53-baf2-19103d5d9bbb"
   },
   "outputs": [],
   "source": [
    "KO_trainer.plot_set('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "XgqN3_Cx4fyv",
    "outputId": "e141ab31-39be-4932-a54c-971b38fac93c"
   },
   "outputs": [],
   "source": [
    "KO_trainer.plot_set('test')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
